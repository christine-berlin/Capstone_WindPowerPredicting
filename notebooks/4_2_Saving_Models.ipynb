{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Saving models\n",
    "In this notebook we save a model (random Forest) for each windfarm for later use with the ```dump()```\n",
    "function of Pickle. Every windfarm has an individual model with the hyperparameters und feature combintaion best\n",
    "for each windfarm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from modeling.functions import train_test_split_features, scaler_func\n",
    "from modeling.features import get_feature_combinations\n",
    "import pickle\n",
    "import ast\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "data = pd.read_csv('../data/GEFCom2014Data/Wind/clean_data.csv')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "data_train = data[data['TIMESTAMP']<='2013-07-01 00:00:00']\n",
    "data_test = data[data['TIMESTAMP']>'2013-07-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model we use to make the predictions\n",
    "model_params = pd.DataFrame()\n",
    "model_params = pd.read_csv(f'../results/RandomForestRegressor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zone in data.ZONEID.unique():\n",
    "    # feature combination\n",
    "    fc = model_params[model_params.ZONE == 'ZONE{}'.format(str(zone))]['FC'].values[0]\n",
    "\n",
    "    # train test split and scale\n",
    "    X_train, X_test, y_train, y_test = train_test_split_features(data_train, data_test, zone, get_feature_combinations()[fc])  \n",
    "    X_train, X_test = scaler_func(X_train, X_train, MinMaxScaler()) \n",
    "\n",
    "    # hyperparameters\n",
    "    best_params = model_params[model_params.ZONE == 'ZONE{}'.format(str(zone))]['BEST_PARAMS'].values[0]\n",
    "  \n",
    "    \n",
    "    # define and fit model\n",
    "    model = RandomForestRegressor()\n",
    "    model.set_params(**ast.literal_eval(best_params))        \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # save the model\n",
    "    pickle.dump(model, open('../models/model{}.pkl'.format(zone), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved the models as .pkl. <br>\n",
    "We have logged the best hyperparamters and feature combinations to mlflow.\n",
    "So the models can be used by either loading the .pkl model, or loading the model from mlfflow to make predictions without training again. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2f7753f64e9a5ea18202ac159afad03a9e7ffdb6bd0b7a7c7ce79153287d031"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
